{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../promptsmith')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanivgal/dev/ai21/promptsmith/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/yanivgal/dev/ai21/promptsmith/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from promptsmith.dspy_init import get_dspy\n",
    "dspy, lm = get_dspy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of floors in the castle David Gregory inherited is not specified in the information provided.\n"
     ]
    }
   ],
   "source": [
    "# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\n",
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the last call to the LLM, with all metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lm.history)  # e.g., 3 calls to the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'messages', 'kwargs', 'response', 'outputs', 'usage', 'cost', 'timestamp', 'uuid', 'model', 'response_model', 'model_type'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.history[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'cost': 8.235e-05,\n",
      "  'kwargs': {},\n",
      "  'messages': [ { 'content': 'Your input fields are:\\n'\n",
      "                             '1. `question` (str)\\n'\n",
      "                             'Your output fields are:\\n'\n",
      "                             '1. `reasoning` (str)\\n'\n",
      "                             '2. `answer` (str)\\n'\n",
      "                             'All interactions will be structured in the '\n",
      "                             'following way, with the appropriate values '\n",
      "                             'filled in.\\n'\n",
      "                             '\\n'\n",
      "                             '[[ ## question ## ]]\\n'\n",
      "                             '{question}\\n'\n",
      "                             '\\n'\n",
      "                             '[[ ## reasoning ## ]]\\n'\n",
      "                             '{reasoning}\\n'\n",
      "                             '\\n'\n",
      "                             '[[ ## answer ## ]]\\n'\n",
      "                             '{answer}\\n'\n",
      "                             '\\n'\n",
      "                             '[[ ## completed ## ]]\\n'\n",
      "                             'In adhering to this structure, your objective '\n",
      "                             'is: \\n'\n",
      "                             '        Given the fields `question`, produce the '\n",
      "                             'fields `answer`.',\n",
      "                  'role': 'system'},\n",
      "                { 'content': '[[ ## question ## ]]\\n'\n",
      "                             'How many floors are in the castle David Gregory '\n",
      "                             'inherited?\\n'\n",
      "                             '\\n'\n",
      "                             'Respond with the corresponding output fields, '\n",
      "                             'starting with the field `[[ ## reasoning ## ]]`, '\n",
      "                             'then `[[ ## answer ## ]]`, and then ending with '\n",
      "                             'the marker for `[[ ## completed ## ]]`.',\n",
      "                  'role': 'user'}],\n",
      "  'model': 'openai/gpt-4o-mini',\n",
      "  'model_type': 'chat',\n",
      "  'outputs': [ '[[ ## reasoning ## ]]\\n'\n",
      "               'The question asks for the number of floors in the castle that '\n",
      "               'David Gregory inherited. However, without specific context or '\n",
      "               'details about the castle in question, it is impossible to '\n",
      "               'provide an accurate answer. The number of floors in a castle '\n",
      "               'can vary widely depending on its design, size, and historical '\n",
      "               'context. \\n'\n",
      "               '\\n'\n",
      "               '[[ ## answer ## ]]\\n'\n",
      "               'The number of floors in the castle David Gregory inherited is '\n",
      "               'not specified in the information provided.\\n'\n",
      "               '\\n'\n",
      "               '[[ ## completed ## ]]'],\n",
      "  'prompt': None,\n",
      "  'response': ModelResponse(id='chatcmpl-BTQwnBfrSQoM4kb9BZpRTKIOmIZP3', created=1746354797, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_dbaca60df0', choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe question asks for the number of floors in the castle that David Gregory inherited. However, without specific context or details about the castle in question, it is impossible to provide an accurate answer. The number of floors in a castle can vary widely depending on its design, size, and historical context. \\n\\n[[ ## answer ## ]]\\nThe number of floors in the castle David Gregory inherited is not specified in the information provided.\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=94, prompt_tokens=173, total_tokens=267, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default', cache_hit=None),\n",
      "  'response_model': 'gpt-4o-mini-2024-07-18',\n",
      "  'timestamp': '2025-05-04T13:33:18.573085',\n",
      "  'usage': { 'completion_tokens': 94,\n",
      "             'completion_tokens_details': CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None),\n",
      "             'prompt_tokens': 173,\n",
      "             'prompt_tokens_details': PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None),\n",
      "             'total_tokens': 267},\n",
      "  'uuid': '42d834e0-6497-444a-9dff-ef6b79258811'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(lm.history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Multiple Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One great aspect of the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval techniques, which enhances retrieval accuracy while significantly speeding up the search process through its late interaction mechanism.',\n",
       " 'One great thing about the ColBERT retrieval model is its ability to combine the strengths of dense and sparse retrieval techniques while maintaining high efficiency through late interaction, allowing for effective large-scale information retrieval.',\n",
       " 'One great aspect of the ColBERT retrieval model is its efficiency in handling large document collections through late interaction, allowing for fast and scalable retrieval without sacrificing accuracy.',\n",
       " 'One great aspect of the ColBERT retrieval model is its efficiency in combining dense and sparse retrieval methods through late interaction, allowing for fast and high-quality document retrieval in large-scale applications.',\n",
       " \"One great aspect of the ColBERT retrieval model is its ability to use BERT's powerful contextual embeddings while allowing for efficient and fast retrieval through late interaction, balancing accuracy and speed effectively.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What's something great about the ColBERT retrieval model?\"\n",
    "\n",
    "answer_a_question = dspy.ChainOfThought('question -> answer', n=5)\n",
    "\n",
    "response = answer_a_question(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.completions.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: The ColBERT retrieval model is notable for its efficiency and effectiveness in handling large-scale information retrieval tasks. It utilizes a two-stage process that combines the benefits of dense and sparse representations, allowing it to efficiently search through vast datasets while maintaining high retrieval accuracy. The model employs late interaction mechanisms, which enable it to compute similarities between query and document embeddings without the need to compute a full dot product for every document, thus significantly speeding up the retrieval process. This makes ColBERT particularly well-suited for applications that require real-time search capabilities, such as search engines.\n",
      "Answer: One great aspect of the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval techniques, which enhances retrieval accuracy while significantly speeding up the search process through its late interaction mechanism.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reasoning: {response.reasoning}\")\n",
    "print(f\"Answer: {response.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check LLM Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai/gpt-4o-mini': {'completion_tokens': 778,\n",
       "  'prompt_tokens': 173,\n",
       "  'total_tokens': 951,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0,\n",
       "   'text_tokens': None},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0,\n",
       "   'cached_tokens': 0,\n",
       "   'text_tokens': None,\n",
       "   'image_tokens': None}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_lm_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mix of relief and disappointment\n",
      "The person likely feels relief from finally being outside after being in a dark room for a long time, indicating a desire for fresh air and light. However, the disappointment comes from the fact that it is raining outside, which may dampen their experience and prevent them from fully enjoying being outdoors.\n"
     ]
    }
   ],
   "source": [
    "feeling_analyzer = dspy.Predict('sentence, situation -> the_actual_feeling_of_the_person_in_the_sentence: str, reasoning: str')\n",
    "\n",
    "sentence=\"i went outside after a long time being in a dark room\"\n",
    "situation=\"it's raining outside\"\n",
    "response = feeling_analyzer(sentence=sentence, situation=situation)\n",
    "\n",
    "print(response.the_actual_feeling_of_the_person_in_the_sentence)\n",
    "print(response.reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: The restructured text preserves the essential meaning of the original input. It maintains the key ideas of going shopping, the items purchased (fruits, vegetables, and pasta), and the fact that cheese was forgotten. The phrase \"to get food\" is a slight generalization but does not distort the overall meaning. The sequence of events is also preserved, with the mention of shopping yesterday and the realization of forgetting cheese. Overall, the changes are acceptable, and the meaning is well-preserved.\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from promptsmith.judges.judge_meaning import JudgeMeaning\n",
    "\n",
    "judge_meaning = dspy.Predict(JudgeMeaning)\n",
    "\n",
    "input_text = (\n",
    "    \"Yesterday, I went to the grocery store to buy ingredients for dinner. \"\n",
    "    \"I ended up buying fruits, vegetables, and pasta. When I got home, I realized I forgot the cheese.\"\n",
    ")\n",
    "\n",
    "output_text = (\n",
    "    \"I went shopping yesterday to get food. I bought some fruits, vegetables, and pasta, \"\n",
    "    \"but forgot to buy cheese.\"\n",
    ")\n",
    "\n",
    "result = judge_meaning(input_text=input_text, output_text=output_text)\n",
    "\n",
    "print(\"Reasoning:\", result.reasoning)\n",
    "print(\"Score:\", result.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Ensemble Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_verdict(verdict):\n",
    "\n",
    "    print(\"\\n📊 Evaluation Results:\")\n",
    "    print(\"----------------------\")\n",
    "    \n",
    "    store = verdict._store\n",
    "\n",
    "    # Find all score, reasoning, and weight fields\n",
    "    score_fields = [k for k in store if k.endswith('_score') and k != 'combined_score']\n",
    "    reasoning_fields = [k for k in store if k.endswith('_reasoning')]\n",
    "    weight_fields = {k.replace('_weight', ''): store[k] for k in store if k.endswith('_weight')}\n",
    "\n",
    "    # Display overall score\n",
    "    overall = store.get('combined_score')\n",
    "    if overall is None and score_fields:\n",
    "        # Fallback: average of all scores\n",
    "        overall = sum(store[k] for k in score_fields) / len(score_fields)\n",
    "    print(f\"\\n🌟 Overall Score: {overall:.3f}\\n\")\n",
    "\n",
    "    # For each judge, display name, score, weight, and reasoning\n",
    "    # Sort for consistent order\n",
    "    for field in sorted(score_fields):\n",
    "        judge_key = field.replace('_score', '')\n",
    "        judge_name = judge_key.replace('_', ' ').title()\n",
    "        reasoning_field = field.replace('_score', '_reasoning')\n",
    "        score = store[field]\n",
    "        reasoning = store.get(reasoning_field, \"\")\n",
    "        weight = weight_fields.get(judge_key, None)\n",
    "        if weight is not None:\n",
    "            print(f\"### {judge_name} Analysis (score={score:.2f}, weight={weight})\")\n",
    "        else:\n",
    "            print(f\"### {judge_name} Analysis (score={score:.2f})\")\n",
    "        print(reasoning)\n",
    "        print()  # Blank line between judges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the original text and the restructured version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Original Text:\n",
      "----------------------\n",
      "I was trying to fix the kitchen sink. At first, I thought it was a clog, but it turned out to be a broken pipe. Water was everywhere, and I had no tools. I called my friend who had some plumbing experience, and he came over. Together we shut off the water and replaced the pipe, which took us the entire afternoon.\n",
      "----------------------\n",
      "\n",
      "📘 Restructured Text:\n",
      "----------------------\n",
      "### Fixing the Kitchen Sink: A Plumbing Adventure\n",
      "\n",
      "Recently, I faced a challenge while trying to fix my kitchen sink. Initially, I suspected that a clog was the issue, but I soon discovered that a broken pipe was the real problem.\n",
      "\n",
      "Water was leaking everywhere, and unfortunately, I didn't have any tools on hand to address the situation. In need of assistance, I called a friend who had some plumbing experience. \n",
      "\n",
      "He quickly came over to help. Together, we managed to shut off the water supply and replace the broken pipe. The entire process took us the whole afternoon, but we were relieved to have resolved the issue.\n",
      "----------------------\n",
      "\n",
      "🧠 Reasoning:\n",
      "----------------------\n",
      "The original text describes a personal experience of fixing a kitchen sink, detailing the initial problem, the discovery of a broken pipe, and the resolution with a friend's help. The restructured version will include a clear title, a brief introduction summarizing the situation, and organized paragraphs that simplify the language while maintaining the original meaning.\n",
      "----------------------\n",
      "\n",
      "🤖 DSPy History:\n",
      "----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-05-04T13:33:32.281656]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `input_text` (str): The original long text.\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `output_text` (str): The restructured text with a title, intro, and clean paragraphs.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## input_text ## ]]\n",
      "{input_text}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## output_text ## ]]\n",
      "{output_text}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        You are a text restructuring expert.\n",
      "        \n",
      "        Your job is to take long, dense text and rewrite it to make it easier to read and understand.\n",
      "        \n",
      "        Instructions:\n",
      "        1. Add a clear and informative title that reflects the topic.\n",
      "        2. Write a short intro paragraph summarizing the overall idea.\n",
      "        3. Break the rest of the text into logically organized paragraphs (one idea per paragraph).\n",
      "        4. Simplify the language where possible, but do not change the meaning.\n",
      "        5. Keep the tone natural and human.\n",
      "        \n",
      "        Do not invent new information. Stay faithful to the original content.\n",
      "        \n",
      "        Return only the restructured version, including the title at the top.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## input_text ## ]]\n",
      "I was trying to fix the kitchen sink. At first, I thought it was a clog, but it turned out to be a broken pipe. Water was everywhere, and I had no tools. I called my friend who had some plumbing experience, and he came over. Together we shut off the water and replaced the pipe, which took us the entire afternoon.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## output_text ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The original text describes a personal experience of fixing a kitchen sink, detailing the initial problem, the discovery of a broken pipe, and the resolution with a friend's help. The restructured version will include a clear title, a brief introduction summarizing the situation, and organized paragraphs that simplify the language while maintaining the original meaning.\n",
      "\n",
      "[[ ## output_text ## ]]\n",
      "### Fixing the Kitchen Sink: A Plumbing Adventure\n",
      "\n",
      "Recently, I faced a challenge while trying to fix my kitchen sink. Initially, I suspected that a clog was the issue, but I soon discovered that a broken pipe was the real problem.\n",
      "\n",
      "Water was leaking everywhere, and unfortunately, I didn't have any tools on hand to address the situation. In need of assistance, I called a friend who had some plumbing experience. \n",
      "\n",
      "He quickly came over to help. Together, we managed to shut off the water supply and replace the broken pipe. The entire process took us the whole afternoon, but we were relieved to have resolved the issue.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "None\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from promptsmith.tasks.restructure_text import RestructureText\n",
    "\n",
    "text_to_restructure = (\n",
    "    \"I was trying to fix the kitchen sink. At first, I thought it was a clog, but it turned out to be a broken pipe. \"\n",
    "    \"Water was everywhere, and I had no tools. I called my friend who had some plumbing experience, and he came over. \"\n",
    "    \"Together we shut off the water and replaced the pipe, which took us the entire afternoon.\"\n",
    ")\n",
    "\n",
    "restructure = dspy.ChainOfThought(RestructureText)\n",
    "restructured_text = restructure(input_text=text_to_restructure)\n",
    "\n",
    "\n",
    "print(\"\\n📝 Original Text:\")\n",
    "print(\"----------------------\")\n",
    "print(text_to_restructure)\n",
    "print(\"----------------------\")\n",
    "\n",
    "print(\"\\n📘 Restructured Text:\")\n",
    "print(\"----------------------\")\n",
    "print(restructured_text.output_text)\n",
    "print(\"----------------------\")\n",
    "\n",
    "print(\"\\n🧠 Reasoning:\")\n",
    "print(restructured_text.reasoning)\n",
    "\n",
    "print(\"\\n🤖 DSPy History:\")\n",
    "print(dspy.inspect_history(n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating the restructured text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptsmith.judges.ensemble_judge import EnsembleJudge\n",
    "import os\n",
    "\n",
    "judge_path = os.path.abspath(\"../promptsmith/judges/judge_restructure_text.yaml\")\n",
    "\n",
    "judge = EnsembleJudge(judge_path)\n",
    "verdict = judge(input_text=text_to_restructure, output_text=restructured_text.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Results:\n",
      "----------------------\n",
      "\n",
      "🌟 Overall Score: 0.990\n",
      "\n",
      "### Focus Relevance Analysis (score=1.00, weight=0.25)\n",
      "The restructured text remains focused on the original message about fixing the kitchen sink. It accurately captures the sequence of events, from suspecting a clog to discovering a broken pipe, and the subsequent actions taken to resolve the issue. The added title and slight rephrasing do not detract from the main ideas, and all content is relevant to the plumbing situation described. There are no off-topic sentences or unnecessary filler, maintaining a clear narrative throughout.\n",
      "\n",
      "### Meaning Analysis (score=1.00, weight=0.25)\n",
      "The restructured text maintains the essential meaning of the original input. Key ideas such as the initial assumption of a clog, the discovery of a broken pipe, the water leak, the lack of tools, and the involvement of a friend with plumbing experience are all preserved. The sequence of events is also retained, including the shutting off of the water and the replacement of the pipe, along with the time it took to complete the task. \n",
      "\n",
      "Minor changes in phrasing and the addition of a title do not distort the overall meaning. The narrative flow is slightly enhanced, making it more engaging without losing any critical details. Therefore, the changes are acceptable.\n",
      "\n",
      "### Readability Analysis (score=0.90, weight=0.1)\n",
      "The text is easy to read due to its clear structure and straightforward language. The sentences are simple and concise, making it easy for the reader to follow the narrative. The paragraphs are logically organized, presenting the problem, the response, and the resolution in a coherent manner. The vocabulary is accessible, avoiding complex terminology, which enhances readability. Overall, the flow from one idea to the next is smooth, contributing to a pleasant reading experience.\n",
      "\n",
      "### Structure Analysis (score=1.00, weight=0.4)\n",
      "The text is well-structured and organized. It begins with a clear and relevant title that sets the context for the narrative. The introductory paragraph effectively summarizes the challenge faced, providing a concise overview of the situation. The main ideas are presented in coherent paragraphs, detailing the problem, the response, and the resolution. The sequence of information flows logically, moving from the initial problem to the actions taken and the eventual outcome. Overall, the structure supports the narrative well, making it easy for the reader to follow the events.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_verdict(verdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### showing few \"meaning\" golden set examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/yanivgal/dev/ai21/promptsmith/promptsmith\n",
      "Loaded 48 dev examples and 12 test examples\n",
      "\n",
      "Example 1:\n",
      "Input: Linda borrowed the book from Mark yesterday.\n",
      "Output: Mark borrowed the book from Linda yesterday.\n",
      "Gold score: 0.6\n",
      "Gold reasoning: Roles reversed; rest identical.\n",
      "\n",
      "Example 2:\n",
      "Input: The novel was written by George Orwell in 1949.\n",
      "Output: The novel was written by J. K. Rowling in 1949.\n",
      "Gold score: 0.2\n",
      "Gold reasoning: Author misattributed; timeframe correct but misleading.\n",
      "\n",
      "Example 3:\n",
      "Input: He earned a salary of $75,000 last year.\n",
      "Output: He earned a salary of $750,000 last year.\n",
      "Gold score: 0.2\n",
      "Gold reasoning: Order‑of‑magnitude hallucination.\n"
     ]
    }
   ],
   "source": [
    "from promptsmith.golden.load import load_golden_set\n",
    "\n",
    "# Load the meaning golden set\n",
    "dev_set, test_set = load_golden_set(judge_name=\"meaning\")\n",
    "\n",
    "print(f\"Loaded {len(dev_set)} dev examples and {len(test_set)} test examples\")\n",
    "def print_examples(examples, n=1):\n",
    "    for i in range(min(n, len(examples))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Input: {examples[i].input_text}\")\n",
    "        print(f\"Output: {examples[i].output_text}\")\n",
    "        print(f\"Gold score: {examples[i].gold_score}\")\n",
    "        print(f\"Gold reasoning: {examples[i].gold_reasoning}\")\n",
    "\n",
    "print_examples(dev_set, n=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calibrating the judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/yanivgal/dev/ai21/promptsmith/promptsmith\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:36:07 INFO dspy.evaluate.evaluate: Average Metric: 38.35000000000001 / 48 (79.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline dev acc: 79.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:36:08 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/05/04 13:36:08 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/05/04 13:36:08 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=3 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/3\n",
      "Bootstrapping set 2/3\n",
      "Bootstrapping set 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]2025/05/04 13:36:11 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'input_text': 'Linda borrowed the book from Mark yesterday.', 'output_text': 'Mark borrowed the book from Linda yesterday.', 'gold_score': 0.6, 'gold_reasoning': 'Roles reversed; rest identical.'}) (input_keys={'input_text', 'output_text'}) with <function score_agreement_metric at 0x1257e4c10> due to getattr(): attribute name must be string.\n",
      " 10%|█         | 1/10 [00:02<00:20,  2.26s/it]2025/05/04 13:36:13 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'input_text': 'The novel was written by George\\xa0Orwell in 1949.', 'output_text': 'The novel was written by J.\\u202fK.\\xa0Rowling in 1949.', 'gold_score': 0.2, 'gold_reasoning': 'Author misattributed; timeframe correct but misleading.'}) (input_keys={'input_text', 'output_text'}) with <function score_agreement_metric at 0x1257e4c10> due to getattr(): attribute name must be string.\n",
      " 20%|██        | 2/10 [00:04<00:16,  2.09s/it]2025/05/04 13:36:15 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'input_text': 'He earned a salary of $75,000 last year.', 'output_text': 'He earned a salary of $750,000 last year.', 'gold_score': 0.2, 'gold_reasoning': 'Order‑of‑magnitude hallucination.'}) (input_keys={'input_text', 'output_text'}) with <function score_agreement_metric at 0x1257e4c10> due to getattr(): attribute name must be string.\n",
      " 30%|███       | 3/10 [00:06<00:14,  2.13s/it]2025/05/04 13:36:17 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'input_text': 'The vaccine trial enrolled 5,000 participants across five countries.', 'output_text': 'The vaccine trial enrolled 4,000 participants across five countries.', 'gold_score': 0.6, 'gold_reasoning': 'Participant number altered; rest accurate.'}) (input_keys={'input_text', 'output_text'}) with <function score_agreement_metric at 0x1257e4c10> due to getattr(): attribute name must be string.\n",
      " 40%|████      | 4/10 [00:08<00:12,  2.00s/it]2025/05/04 13:36:19 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'input_text': 'The spacecraft entered Mars orbit after a seven‑month journey.', 'output_text': 'The spacecraft landed on Mars after a seven‑month journey.', 'gold_score': 0.4, 'gold_reasoning': 'Orbit ↔ landing; related but major distinction.'}) (input_keys={'input_text', 'output_text'}) with <function score_agreement_metric at 0x1257e4c10> due to getattr(): attribute name must be string.\n",
      " 50%|█████     | 5/10 [00:10<00:10,  2.14s/it]2025/05/04 13:36:21 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'input_text': 'Regular exercise improves cardiovascular health.', 'output_text': 'Regular exercise is the leading cause of heart disease.', 'gold_score': 0.2, 'gold_reasoning': 'Direct contradiction of central statement.'}) (input_keys={'input_text', 'output_text'}) with <function score_agreement_metric at 0x1257e4c10> due to getattr(): attribute name must be string.\n",
      " 60%|██████    | 6/10 [00:12<00:08,  2.15s/it]2025/05/04 13:36:23 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'input_text': 'The museum opens at 10\\u202fa.m.', 'output_text': 'The museum opens at 10\\u202fp.m.', 'gold_score': 0.4, 'gold_reasoning': 'Confuses a.m. with p.m.'}) (input_keys={'input_text', 'output_text'}) with <function score_agreement_metric at 0x1257e4c10> due to getattr(): attribute name must be string.\n",
      " 70%|███████   | 7/10 [00:14<00:06,  2.10s/it]2025/05/04 13:36:26 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'input_text': 'When Rachel finished the project, she emailed it to her manager.', 'output_text': 'After completing the project, Rachel sent it to her supervisor via email.', 'gold_score': 1.0, 'gold_reasoning': 'All entities and actions preserved.'}) (input_keys={'input_text', 'output_text'}) with <function score_agreement_metric at 0x1257e4c10> due to getattr(): attribute name must be string.\n",
      " 80%|████████  | 8/10 [00:17<00:04,  2.15s/it]2025/05/04 13:36:28 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'input_text': 'He ran a marathon in 3\\xa0hours and 45\\xa0minutes last Sunday.', 'output_text': 'He ran a marathon last Sunday.', 'gold_score': 0.8, 'gold_reasoning': 'Time detail dropped; core event intact.'}) (input_keys={'input_text', 'output_text'}) with <function score_agreement_metric at 0x1257e4c10> due to getattr(): attribute name must be string.\n",
      " 90%|█████████ | 9/10 [00:21<00:02,  2.41s/it]\n",
      "2025/05/04 13:36:30 INFO dspy.teleprompt.mipro_optimizer_v2: Error generating few-shot examples: getattr(): attribute name must be string\n",
      "2025/05/04 13:36:30 INFO dspy.teleprompt.mipro_optimizer_v2: Running without few-shot examples.\n",
      "2025/05/04 13:36:30 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/05/04 13:36:30 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "2025/05/04 13:36:38 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing instructions...\n",
      "\n",
      "2025/05/04 13:37:03 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/05/04 13:37:03 INFO dspy.teleprompt.mipro_optimizer_v2: 0: You are an expert in evaluating meaning preservation.\n",
      "\n",
      "Your task is to determine whether the restructured text preserves the essential meaning of the original input text.\n",
      "\n",
      "Please consider:\n",
      "1. Are all key ideas and important details still present?\n",
      "2. Were any important parts of the original meaning lost or changed significantly?\n",
      "3. Are minor rephrasings or small generalizations acceptable if they don't distort the overall meaning?\n",
      "\n",
      "Assign a score between 0 (poor preservation) and 1 (perfect preservation).\n",
      "\n",
      "Explain your reasoning clearly, mentioning what was preserved well and any important losses or changes.\n",
      "\n",
      "2025/05/04 13:37:03 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are a language evaluation expert tasked with assessing the fidelity of paraphrased text. Your job is to determine whether the restructured text maintains the essential meaning of the original input. Please analyze the following:\n",
      "\n",
      "1. Are all key ideas and important details preserved?\n",
      "2. Have any significant elements of the original meaning been altered or lost?\n",
      "3. Are there acceptable minor rephrasings or generalizations that do not distort the overall meaning?\n",
      "\n",
      "Assign a score from 0 (indicating poor preservation) to 1 (indicating perfect preservation). Provide a clear explanation of your reasoning, highlighting what was preserved effectively and noting any critical losses or changes.\n",
      "\n",
      "2025/05/04 13:37:03 INFO dspy.teleprompt.mipro_optimizer_v2: 2: You are a specialist in paraphrasing evaluation. Your task is to assess whether the paraphrased text maintains the core meaning of the original input. Please evaluate the following:\n",
      "\n",
      "1. Identify if all key concepts and critical details from the original text are retained.\n",
      "2. Analyze if any significant alterations or losses occurred that affect the original meaning.\n",
      "3. Consider if slight modifications or generalizations are permissible as long as they do not misrepresent the overall message.\n",
      "\n",
      "Provide a score from 0 (indicating poor preservation) to 1 (indicating perfect preservation). Additionally, offer a detailed explanation of your assessment, highlighting what aspects of the meaning were preserved effectively and noting any significant losses or misinterpretations.\n",
      "\n",
      "2025/05/04 13:37:03 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/05/04 13:37:03 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/05/04 13:37:03 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/05/04 13:37:03 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 7 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 31.25 / 38 (82.2%): 100%|██████████| 38/38 [00:13<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:37:16 INFO dspy.evaluate.evaluate: Average Metric: 31.25 / 38 (82.2%)\n",
      "2025/05/04 13:37:16 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 82.24\n",
      "\n",
      "/Users/yanivgal/dev/ai21/promptsmith/venv/lib/python3.9/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/05/04 13:37:16 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 2 / 7 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 27.90 / 35 (79.7%): 100%|██████████| 35/35 [00:12<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:37:29 INFO dspy.evaluate.evaluate: Average Metric: 27.900000000000002 / 35 (79.7%)\n",
      "2025/05/04 13:37:29 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 79.71 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1'].\n",
      "2025/05/04 13:37:29 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [79.71]\n",
      "2025/05/04 13:37:29 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [82.24]\n",
      "2025/05/04 13:37:29 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 82.24\n",
      "2025/05/04 13:37:29 INFO dspy.teleprompt.mipro_optimizer_v2: ========================================\n",
      "\n",
      "\n",
      "2025/05/04 13:37:29 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 3 / 7 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 28.95 / 35 (82.7%): 100%|██████████| 35/35 [00:11<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:37:40 INFO dspy.evaluate.evaluate: Average Metric: 28.95 / 35 (82.7%)\n",
      "2025/05/04 13:37:40 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 82.71 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0'].\n",
      "2025/05/04 13:37:40 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [79.71, 82.71]\n",
      "2025/05/04 13:37:40 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [82.24]\n",
      "2025/05/04 13:37:40 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 82.24\n",
      "2025/05/04 13:37:40 INFO dspy.teleprompt.mipro_optimizer_v2: ========================================\n",
      "\n",
      "\n",
      "2025/05/04 13:37:40 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 4 / 7 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 28.80 / 35 (82.3%): 100%|██████████| 35/35 [00:11<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:37:51 INFO dspy.evaluate.evaluate: Average Metric: 28.8 / 35 (82.3%)\n",
      "2025/05/04 13:37:51 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 82.29 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1'].\n",
      "2025/05/04 13:37:51 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [79.71, 82.71, 82.29]\n",
      "2025/05/04 13:37:51 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [82.24]\n",
      "2025/05/04 13:37:51 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 82.24\n",
      "2025/05/04 13:37:51 INFO dspy.teleprompt.mipro_optimizer_v2: ========================================\n",
      "\n",
      "\n",
      "2025/05/04 13:37:51 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 5 / 7 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 28.00 / 35 (80.0%): 100%|██████████| 35/35 [00:11<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:38:02 INFO dspy.evaluate.evaluate: Average Metric: 28.000000000000004 / 35 (80.0%)\n",
      "2025/05/04 13:38:02 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 80.0 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2'].\n",
      "2025/05/04 13:38:02 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [79.71, 82.71, 82.29, 80.0]\n",
      "2025/05/04 13:38:02 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [82.24]\n",
      "2025/05/04 13:38:02 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 82.24\n",
      "2025/05/04 13:38:02 INFO dspy.teleprompt.mipro_optimizer_v2: ========================================\n",
      "\n",
      "\n",
      "2025/05/04 13:38:02 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 6 / 7 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 28.25 / 35 (80.7%): 100%|██████████| 35/35 [00:12<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:38:15 INFO dspy.evaluate.evaluate: Average Metric: 28.250000000000007 / 35 (80.7%)\n",
      "2025/05/04 13:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 80.71 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0'].\n",
      "2025/05/04 13:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [79.71, 82.71, 82.29, 80.0, 80.71]\n",
      "2025/05/04 13:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [82.24]\n",
      "2025/05/04 13:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 82.24\n",
      "2025/05/04 13:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: ========================================\n",
      "\n",
      "\n",
      "2025/05/04 13:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 7 - Full Evaluation =====\n",
      "2025/05/04 13:38:15 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 81.71) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 31.30 / 38 (82.4%): 100%|██████████| 38/38 [00:13<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:38:29 INFO dspy.evaluate.evaluate: Average Metric: 31.3 / 38 (82.4%)\n",
      "2025/05/04 13:38:29 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mNew best full eval score!\u001b[0m Score: 82.37\n",
      "2025/05/04 13:38:29 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [82.24, 82.37]\n",
      "2025/05/04 13:38:29 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 82.37\n",
      "2025/05/04 13:38:29 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/05/04 13:38:29 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/05/04 13:38:29 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 82.37!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/04 13:38:44 INFO dspy.evaluate.evaluate: Average Metric: 38.25000000000001 / 48 (79.7%)\n",
      "2025/05/04 13:38:44 ERROR dspy.utils.parallelizer: Error for Example({'input_text': '\"Don\\'t forget the keys,\" Maria reminded John.', 'output_text': 'Maria reminded John that he had already lost the keys.', 'gold_score': 0.4, 'gold_reasoning': 'Adds new information that changes meaning.'}) (input_keys={'input_text', 'output_text'}): 'tuple' object is not callable. Set `provide_traceback=True` for traceback.\n",
      "2025/05/04 13:38:44 ERROR dspy.utils.parallelizer: Error for Example({'input_text': 'E = mc² describes the equivalence of mass and energy.', 'output_text': 'E = mc² proves mass and time are equivalent.', 'gold_score': 0.2, 'gold_reasoning': 'Core concept distorted.'}) (input_keys={'input_text', 'output_text'}): 'tuple' object is not callable. Set `provide_traceback=True` for traceback.\n",
      "2025/05/04 13:38:44 ERROR dspy.utils.parallelizer: Error for Example({'input_text': 'The library opens at 9\\u202fa.m. and closes at 6\\u202fp.m. on weekdays.', 'output_text': 'The library is open on weekdays from morning until evening.', 'gold_score': 0.8, 'gold_reasoning': 'Precise hours softened; general meaning kept.'}) (input_keys={'input_text', 'output_text'}): 'tuple' object is not callable. Set `provide_traceback=True` for traceback.\n",
      "2025/05/04 13:38:44 ERROR dspy.utils.parallelizer: Error for Example({'input_text': 'There are 1,024\\xa0megabytes in a gigabyte.', 'output_text': 'There are 1,024\\xa0kilobytes in a gigabyte.', 'gold_score': 0.6, 'gold_reasoning': 'Unit error; number correct.'}) (input_keys={'input_text', 'output_text'}): 'tuple' object is not callable. Set `provide_traceback=True` for traceback.\n",
      "2025/05/04 13:38:44 ERROR dspy.utils.parallelizer: Error for Example({'input_text': 'Mount\\xa0Everest rises 8,849\\u202fmeters above sea level.', 'output_text': 'Mount\\xa0Everest is about 8.8\\u202fkilometers tall.', 'gold_score': 0.8, 'gold_reasoning': 'Rounding introduces tiny numeric error, acceptable.'}) (input_keys={'input_text', 'output_text'}): 'tuple' object is not callable. Set `provide_traceback=True` for traceback.\n",
      "2025/05/04 13:38:44 WARNING dspy.utils.parallelizer: Execution cancelled due to errors or interruption.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned dev acc: 79.690\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Execution cancelled due to errors or interruption.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m best_judge \u001b[38;5;241m=\u001b[39m calibrate_judge(devset\u001b[38;5;241m=\u001b[39mdevset, judge_class\u001b[38;5;241m=\u001b[39mJudgeMeaning)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 3. Evaluate on held‑out examples\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHeld‑out accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mevaluate_judge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_judge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 4. Inspect the optimiser’s final prompt\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m— Tuned prompt text —\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/ai21/promptsmith/promptsmith/judges/metrics.py:33\u001b[0m, in \u001b[0;36mevaluate_judge\u001b[0;34m(judge_module, testset, metric)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_judge\u001b[39m(judge_module, testset, metric\u001b[38;5;241m=\u001b[39mscore_agreement_metric):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return accuracy on a held‑out test set.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEvaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjudge_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/ai21/promptsmith/venv/lib/python3.9/site-packages/dspy/utils/callback.py:326\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.sync_wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _get_active_callbacks(instance)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n\u001b[1;32m    330\u001b[0m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "File \u001b[0;32m~/dev/ai21/promptsmith/venv/lib/python3.9/site-packages/dspy/evaluate/evaluate.py:170\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs, callback_metadata)\u001b[0m\n\u001b[1;32m    166\u001b[0m         program\u001b[38;5;241m.\u001b[39m_suggest_failures \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggest_failures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction, score\n\u001b[0;32m--> 170\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(results)\n\u001b[1;32m    173\u001b[0m results \u001b[38;5;241m=\u001b[39m [((dspy\u001b[38;5;241m.\u001b[39mPrediction(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfailure_score) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/dev/ai21/promptsmith/venv/lib/python3.9/site-packages/dspy/utils/parallelizer.py:48\u001b[0m, in \u001b[0;36mParallelExecutor.execute\u001b[0;34m(self, function, data)\u001b[0m\n\u001b[1;32m     46\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mtqdm\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     47\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_function(function)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/ai21/promptsmith/venv/lib/python3.9/site-packages/dspy/utils/parallelizer.py:203\u001b[0m, in \u001b[0;36mParallelExecutor._execute_parallel\u001b[0;34m(self, function, data)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_jobs\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m    202\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution cancelled due to errors or interruption.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution cancelled due to errors or interruption.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mException\u001b[0m: Execution cancelled due to errors or interruption."
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import promptsmith.judges.judge_meaning\n",
    "importlib.reload(promptsmith.judges.judge_meaning)\n",
    "\n",
    "import promptsmith.judges.metrics\n",
    "importlib.reload(promptsmith.judges.metrics)\n",
    "\n",
    "from promptsmith.judges.metrics import calibrate_judge, evaluate_judge\n",
    "\n",
    "from promptsmith.golden.load import load_golden_set\n",
    "\n",
    "\n",
    "# 1. Load and split the golden set\n",
    "devset, testset = load_golden_set(\"meaning\")\n",
    "\n",
    "# 2. Optimise the prompt (auto='heavy' tries more variants)\n",
    "best_judge = calibrate_judge(devset=devset, judge_class=JudgeMeaning)\n",
    "\n",
    "# 3. Evaluate on held‑out examples\n",
    "print(\"\\nHeld‑out accuracy:\", evaluate_judge(best_judge, testset))\n",
    "\n",
    "# 4. Inspect the optimiser’s final prompt\n",
    "print(\"\\n— Tuned prompt text —\\n\")\n",
    "print(best_judge.signature.instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Original prompt —\n",
      "==============================\n",
      "\n",
      "You are an expert in evaluating meaning preservation.\n",
      "\n",
      "Your task is to determine whether the restructured text preserves the essential meaning of the original input text.\n",
      "\n",
      "Please consider:\n",
      "1. Are all key ideas and important details still present?\n",
      "2. Were any important parts of the original meaning lost or changed significantly?\n",
      "3. Are minor rephrasings or small generalizations acceptable if they don't distort the overall meaning?\n",
      "\n",
      "Assign a score between 0 (poor preservation) and 1 (perfect preservation).\n",
      "\n",
      "Explain your reasoning clearly, mentioning what was preserved well and any important losses or changes.\n",
      "\n",
      "— Tuned prompt —\n",
      "==============================\n",
      "\n",
      "You are a language evaluation expert tasked with assessing the quality of sentence transformations. Your job is to determine if the transformed text maintains the essential meaning of the original input sentence. \n",
      "\n",
      "When evaluating, please consider the following:\n",
      "1. Are all key ideas and important details from the original sentence still present in the transformed version?\n",
      "2. Have any significant parts of the original meaning been lost or altered?\n",
      "3. Are minor rephrasings or generalizations acceptable if they do not distort the overall meaning?\n",
      "\n",
      "Assign a score between 0 (poor preservation) and 1 (perfect preservation) based on your analysis. Provide a clear explanation of your reasoning, highlighting what aspects were preserved effectively and noting any critical losses or changes.\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from judges.judge_meaning import JudgeMeaning \n",
    "\n",
    "# original (baseline) predictor\n",
    "baseline = dspy.Predict(JudgeMeaning)\n",
    "\n",
    "print(\"— Original prompt —\")\n",
    "print(\"=\" * 30)\n",
    "print()\n",
    "print(baseline.signature.instructions)\n",
    "\n",
    "print(\"\\n— Tuned prompt —\")\n",
    "print(\"=\" * 30)\n",
    "print()\n",
    "print(best_judge.signature.instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_judge.save(\"best_meaning_judge.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
